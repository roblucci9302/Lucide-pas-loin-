================================================================================
                 LUCIDE LLM TOKEN CONSUMPTION ARCHITECTURE
                             ANALYSIS SUMMARY
================================================================================

GENERATED: 2025-11-11
LOCATION: /home/user/Lucide-101214
STATUS: COMPREHENSIVE EXPLORATION COMPLETE

================================================================================
                               KEY FINDINGS
================================================================================

1. FACTORY ARCHITECTURE (COMPLETE)
   - Central location: /src/features/common/ai/factory.js
   - Supports 6 providers: OpenAI, Anthropic, Gemini, Ollama, Deepgram, Whisper
   - Creates LLM and STT instances
   - All providers working correctly

2. LLM PROVIDERS (COMPLETE)
   - OpenAI: ✓ Token data available in response.raw.usage
   - Anthropic: ✓ Token data available (different field names)
   - Gemini: ⚠️ Token data partial/inconsistent
   - Ollama: ✗ No token tracking (local model)
   - Streaming responses: ✗ No usage data (API limitation)

3. CALL SITES (3 IDENTIFIED)
   a) Ask Service (Line 396)
      - Type: STREAMING (main chat interaction)
      - Issue: Streaming has NO token data
      - Status: No tracking implemented
      
   b) Summary Service (Line 176)
      - Type: NON-STREAMING (meeting summary)
      - Opportunity: Token data AVAILABLE but not captured
      - Status: No tracking implemented (EASIEST TO FIX)
      
   c) Response Service (Line 158)
      - Type: NON-STREAMING (suggestion generation)
      - Opportunity: Token data AVAILABLE but not captured
      - Status: No tracking implemented

4. DATABASE SCHEMA (READY BUT UNUSED)
   - ai_messages.tokens (INTEGER) - DEFINED BUT NEVER POPULATED
   - summaries.tokens_used (INTEGER) - DEFINED BUT NEVER POPULATED
   - Schema designed but repositories don't support token params

5. CURRENT TRACKING STATUS
   - Token schema: DEFINED
   - Repository implementation: MISSING
   - Service integration: MISSING
   - Cost calculation: MISSING
   - Usage dashboard: MISSING
   
   => Infrastructure exists but NO actual tracking happening

================================================================================
                            CRITICAL GAPS
================================================================================

1. STREAMING LIMITATION (Ask Service)
   - Main chat uses streaming (fast, real-time)
   - Streaming API responses don't include token usage
   - Solution needed for primary use case

2. REPOSITORY FUNCTIONS DON'T ACCEPT TOKENS
   - addAiMessage() signature needs tokens parameter
   - saveSummary() signature needs tokens_used parameter
   - Both backward compatible but currently unused

3. NO TOKEN EXTRACTION SERVICE
   - Each provider returns tokens differently
   - Need normalization layer
   - No centralized extraction logic exists

4. PROVIDER FIELD INCONSISTENCY
   - OpenAI: response.usage.prompt_tokens
   - Anthropic: response.usage.input_tokens
   - Gemini: result.usageMetadata.promptTokenCount
   - Ollama: N/A

5. NO COST CALCULATION
   - Pricing config doesn't exist
   - No cost estimation
   - No budget tracking

================================================================================
                        RECOMMENDED QUICK WINS
================================================================================

PHASE 1: ENABLE TOKEN CAPTURE (2-3 hours)
1. Create tokenTrackingService.js
   - extractTokenUsage(provider, response)
   - Normalize all providers to: {prompt_tokens, completion_tokens, total_tokens}
   
2. Update repositories to accept tokens
   - Add optional parameters to addAiMessage()
   - Add optional parameters to saveSummary()
   
3. Integrate in Summary Service (Line 176)
   - Already uses non-streaming (has token data)
   - Minimal changes needed
   - EASIEST INTEGRATION POINT

PHASE 2: COST ESTIMATION (1-2 hours)
1. Create pricingConfig.js
2. Add calculateCost() function
3. Link to token tracking

PHASE 3: HANDLE STREAMING GAP (1-2 hours)
Choose one solution:
- Option A: Estimate tokens from actual streamed content
- Option B: Fall back to non-streaming for Ask service
- Option C: Use Portkey middleware

================================================================================
                          FILES SAVED
================================================================================

Two comprehensive documents created:

1. LLM_TOKEN_TRACKING_ANALYSIS.md (612 lines)
   - Complete architecture analysis
   - Provider response formats
   - Implementation roadmap
   - Code examples
   - Risk assessment

2. QUICK_REFERENCE_TOKEN_TRACKING.md (400 lines)
   - Line-by-line call sites
   - Field mapping for each provider
   - Repository signatures
   - Priority tasks
   - Next steps

================================================================================
                      CALL SITE LOCATIONS
================================================================================

ASK SERVICE
  File: /home/user/Lucide-101214/src/features/ask/askService.js
  Line 396: streamingLLM.streamChat(messages) - PRIMARY CHAT
  Issue: Streaming, no token data
  
SUMMARY SERVICE
  File: /home/user/Lucide-101214/src/features/listen/summary/summaryService.js
  Line 176: llm.chat(messages) - SUMMARY GENERATION
  Status: Non-streaming, tokens available but not captured
  
RESPONSE SERVICE
  File: /home/user/Lucide-101214/src/features/listen/response/responseService.js
  Line 158: llm.chat(messages) - SUGGESTIONS
  Status: Non-streaming, tokens available but not captured

================================================================================
                    PROVIDER RESPONSE STRUCTURES
================================================================================

OPENAI (response.raw)
  ├─ usage.prompt_tokens      ✓
  ├─ usage.completion_tokens  ✓
  └─ usage.total_tokens       ✓
  
ANTHROPIC (response.raw)
  ├─ usage.input_tokens       ✓ (maps to prompt_tokens)
  ├─ usage.output_tokens      ✓ (maps to completion_tokens)
  └─ [need to compute total]
  
GEMINI (response.raw)
  ├─ usageMetadata.promptTokenCount           ⚠️ (inconsistent)
  ├─ usageMetadata.candidatesTokenCount       ⚠️
  └─ usageMetadata.totalTokenCount            ⚠️
  
OLLAMA (response.raw)
  └─ [no token data]

STREAMING (all providers)
  └─ [no usage in response]

================================================================================
                     DATABASE SCHEMA STATUS
================================================================================

Table: ai_messages
  Column: tokens (INTEGER)
  Status: DEFINED IN SCHEMA
  Population: NEVER HAPPENS
  Populated By: (currently nobody)
  
Table: summaries  
  Column: tokens_used (INTEGER)
  Status: DEFINED IN SCHEMA
  Population: NEVER HAPPENS
  Populated By: (currently nobody)

Schema File: /src/features/common/config/schema.js
  Lines 44-55: ai_messages definition
  Lines 57-69: summaries definition

================================================================================
                      REPOSITORY STATUS
================================================================================

Ask Repository: /src/features/ask/repositories/sqlite.repository.js
  Function: addAiMessage()
  Lines: 3-17
  Parameters: {uid, sessionId, role, content, model}
  Missing: tokens parameter ✗

Summary Repository: /src/features/listen/summary/repositories/sqlite.repository.js
  Function: saveSummary()
  Lines: 3-28
  Parameters: {uid, sessionId, tldr, text, bullet_json, action_json, model}
  Missing: tokens_used parameter ✗

================================================================================
                     TOKEN UTILITIES (ESTIMATION)
================================================================================

File: /src/features/common/utils/tokenUtils.js
  estimateTokens(text) - ⚠️ ESTIMATION ONLY
    Heuristic: ~4 characters per token
    Accuracy: ±10-20%
    Use: Fallback when actual tokens unavailable
  
Note: These are estimates, not actual API token usage

================================================================================
                       IMPLEMENTATION ROADMAP
================================================================================

IMMEDIATE (Week 1)
  [ ] Create tokenTrackingService.js
      - extractTokenUsage(provider, response)
      - Handle field mapping for all providers
      
  [ ] Update askRepository.addAiMessage() signature
      - Add optional: tokens, prompt_tokens, completion_tokens
      
  [ ] Update summaryRepository.saveSummary() signature
      - Add optional: tokens_used, prompt_tokens, completion_tokens

SHORT-TERM (Week 2)
  [ ] Integrate in Summary Service (Line 176)
      - Extract tokens from completion
      - Pass to saveSummary()
      
  [ ] Integrate in Response Service (Line 158) - Optional
      - Track in-memory for analytics

MEDIUM-TERM (Week 3)
  [ ] Handle Ask Service streaming gap
      - Choose estimation, fallback, or Portkey approach
      
  [ ] Create pricingConfig.js
      - Add pricing for OpenAI, Anthropic, Gemini
      
  [ ] Build usage statistics repository
      - Aggregate by user, provider, model, date

LONG-TERM (Month 2)
  [ ] Usage dashboard
  [ ] Cost alerts and budgets
  [ ] Provider recommendations
  [ ] Enterprise billing integration

================================================================================
                         RISK ASSESSMENT
================================================================================

UNBOUNDED USAGE RISK: HIGH
  - Without tracking, users could incur large unexpected costs
  - No visibility into API spending
  - Streaming chat (primary feature) completely untracked

STREAMING GAP RISK: CRITICAL
  - Ask Service (main feature) uses streaming
  - Streaming API doesn't return token usage
  - Main interaction method has no token tracking

SCHEMA ORPHANING RISK: MEDIUM
  - Token columns exist in schema but never populated
  - Could cause confusion for future developers
  - Migration overhead to add later

PROVIDER INCONSISTENCY RISK: MEDIUM
  - Different field names across providers
  - Potential bugs in normalization layer
  - Testing needed for each provider

================================================================================
                              NEXT ACTIONS
================================================================================

1. Review both analysis documents
   - LLM_TOKEN_TRACKING_ANALYSIS.md (comprehensive)
   - QUICK_REFERENCE_TOKEN_TRACKING.md (quick lookup)

2. Prioritize by impact
   - Priority 1: Summary Service (easiest, non-streaming)
   - Priority 2: Response Service (ephemeral, lower priority)
   - Priority 3: Ask Service (complex, streaming limitation)

3. Create tokenTrackingService.js
   - Simple provider-agnostic extraction

4. Update repositories
   - Minimal signature changes, backward compatible

5. Test with all providers
   - OpenAI, Anthropic, Gemini, Ollama

6. Plan streaming solution
   - Decide between estimate/fallback/Portkey

================================================================================
                            CONCLUSION
================================================================================

Lucide has a well-architected LLM factory with strong provider support. However,
token tracking infrastructure (schema + repositories) is defined but not
implemented. 

Token usage data IS AVAILABLE from all non-streaming providers but is currently
being DISCARDED. Two quick wins exist:

1. Summary Service: Non-streaming, full token data available
2. Response Service: Non-streaming, full token data available

Only Ask Service requires additional work due to streaming limitation, which
can be addressed with token estimation or provider-level solutions.

Implementing full token tracking across all services is achievable in 3-4 hours
for phases 1-2, with medium-term work needed for streaming solution and
analytics.

================================================================================
